---
title: "Natural Language Processing in R: A Wikipedia Case Study"
author: "Jack Wilburn and Rob Squire"
date: "4/30/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(cache=TRUE)
```

### Abstract

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
The purpose of this semester long project was to learn a new skill in R: Natural Language Processing (NLP). The impetus for this project came from Kaggle.com, a site that hosts data science related challenges, in the form of a competition that asked us to classify certain Wikipedia submission comments into 6 categories (toxic, severe toxic, obscene, insult identity hate, and threat). Our objectives were to understand what NLP is and to create a cross validated logistic regression model that would accurately classify the comment text based on these categories. We were successful in both endeavours and were able to predict comment categories with as much as 31.29% improvement over the no-information rate, significant for us but not the best entry in the competition. This newly gained knowledge will help us when formulating other NLP based predictive models and the research methodology we used will help us learn other techniques for future model building in any setting. 

**Keywords: Confusion Matrix, Corpus, Cross Validation, Document Feature Matrix, Logistic Regression, Natural Language Processing (NLP), Sentiment Analysis, Sparse Matrix, Text Mining, Tokens**

### Introduction

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
Spoken and written language have been the predominant method for conveying ideas, stories, thoughts, etc. for milenia. In fact, “language first appeared among Homo sapiens somewhere between 30,000 and 100,000 years ago” (Bryant), and is still our principal method of communication.  In any context, language necessitates an accurate comprehension of a message; however, the dissemination of ideas has far outpaced our interpretive abilities (insideBIGDATA). As a consequence, there are large bodies of knowledge that lie unread or unheard and thus that knowledge has not been co-opted into society.  The ability to condense a body of text from: Twitter tweets, books, internet comments, websites, etc, is vital to the future of drawing meaningful information from vasts amount of text that would be infeasible to read. By condensation, we mean that we want to reduce the size of the information from millions of words to fewer variables that have some predictive/interpretive properties. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
With the onset of computers, we are now on the precipice of a new method for language comprehension at speeds that far outpace human ability.  Developing methods for computers to interpret language will allow for the extraction of key information from vast bodies of text that was previously unextractable. There have been methods in place for well over 50 years (Hutchins) but the underlying manipulations and goals remain almost exactly the same. In fact, one of the first major forays into this field was from the Terry Winograd from MIT who created a computer sandbox that had users input commands in plain, human readable text. The commands could move blocks and allowed users to say things like “grab the largest block” and the program would do so. Even this seemingly basic task was so complicated that Winograd published his results in the well established journal,  Cognitive Psychology. This first exploration into the field saw Winograd hard coding in specific commands but, of course, he couldn’t account for every possible command and thus the door had opened for research into NLP. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
While Winograd’s results were monumental, they still fell very short of true, text comprehension. Winograd was simply working with some compound commands that could be accessed a human using plain text. This left some room for future researchers to explore new spaces in which users interacted in a truly natural way with a computer. One of the most pertinent and socially effusive examples of this is CleverBot. CleverBot was an application of AI that built its’ response databases from other humans answers. Explicitly it say on their site “Cleverbot learns from people - things it says may seem inappropriate - use with discretion and at YOUR OWN RISK,” (CleverBot) showing that there is the potential for the bot to learn behaviors that are potentially offensive or not acceptable in our society. This show us that the bot might not truly be understanding the conversation and that the responses aren’t carefully crafted. Thus we’re still not there yet and accurate and responsive language processing is still a future goal.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
Since the Winograd program there have been a multitude of explorations into NLP but previous motivations have mostly been academic – people mostly want to create artificial intelligence. Thus with digital marketing and online shopping, researchers and data scientists want to understand larger bodies of text so that they may draw out inference from product descriptions, twitter comments, and user reviews. The hope is that drawing inference from these strings of text will allow marketers and data scientists to better predict when/if people will buy products and to predict the price that consumers will pay. Additionally, researchers and data scientists are interested in automating time intensive tasks such as forum moderation and user comment understanding. This was our motivation and we explore whether or not we can speed up user comment understanding. 

### Methods

**Natural Language Processing (NLP)**
    
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
At a basic level, natural language processing is defined as “the automatic manipulation of natural language, like speech and text, by software” (Brownlee). The goal of NLP is to develop a system of breaking down bodies of texts into a format that a computer can understand.  Once this understanding is established, we can make use of the computer to manipulate language for desired exploration or predictions. This includes breaking down strings of text such as comments, product reviews, books, etc. in such a way that a computer can reasonably interpre. In so doing, we assign values to particular text features that serve to create informative, or even predictive, variables that help explain specific properties of the text we’re interested in. (Chowdhury, 51). NLP is colloquially referred to as text mining and is the backbone of this project. We applied NLP extensively to clean, process, and represent text strings from Wikipedia comments as new variables that may have predictive power. NLP was appropriate as the text data we were working with was generated by humans interacting with other Wikipedia users and editors by a comment text submission box. The variables created are included later in this section and facilitated predictions of whether or not comments would be objectively insulting, toxic, or hateful.
    
**Variable Creation**
    
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
Variable creation is imperative to NLP and was vital to this study.  Extracting numerical and categorical variables from plain text was a lengthy process that consumed a lot of computation time. This process resulted in 4028 variables that were used in evaluating text. The variables included:

  - length (number of characters)
  - ncap (number of capital letters),
  - nnum (number of numeric characters)
  - ncap_len(ratio of ncap to Length)
  - nnum_len (ratio of nnum to length),
  - nexcl (number of exclamation marks)
  - nquest (number of question marks)
  - npunct (number of punctuation marks)
  - npunct_len (ratio of punctuation to length)
  - nsent (number of sentences)
  - nsymb (number of symbols from “ (&|@|#|\\$|%|\\*|\\^ )”)
  - nsmile (number of smiley faces from “(?::|;|=)(?:-)?(?:\\)|D|P)”)
  - nwords (number of words)
  - hate (number of occurrences of “ni****” and “fa*”)
  - nhate (number of occurrences of hate words defined by custom lexicon)
  - AFINN sentiment value (described later)
  - Custom swear sentiment value (described later)
  - Hate words sentiment value (described later)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
The remaining 4000 variables were counts of different single word components of the comment text.  We discuss this structure later under document feature matrix and sparse matrix. 

**Tokens**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
A token, put simply, is a word within a string of text. When a token appears in a string it only counts as one word even if there are multiple occurrences. For example, in the string “I love love data.”  there are only 3 tokens, “I”, “love”, and “data.” This is the convention of the tidy text package (Silge) and the function tokenize (Wickham). Tokens are useful in understanding which words are present and how many strings in a list of strings contain the word, but not for understanding total occurrences in either one string or the entire string list. Tokenizing is useful for identifying unique words for each of the categories and provides a basic level of context that a computer can interpret quickly and efficiently. Manipulating tokens is useful in creating predictive variables necessary for accurately classifying comments.

**N-grams**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
A n-gram is similar to a token but is a string comprising of “n” words. For example, the words “love” and “you” often occur consecutively so instead of searching for each word as a token, the bi-gram “love you” is more contextually meaningful. This aids in identifying sentiment for groupings of words as opposed to if you searched for each word independently. Utilizing n-grams in addition to the other variables will engender an increase in model accuracy. An attempt at n-gram analysis was unsuccessful due to computing limitations so this technique was not employed but is referenced as a potential area of future research.

**Sentiment Analysis**
    
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
Sentiment analysis consists of assigning values to words, tokens, or n-grams in order to classify/interpret a specific body of text. Sentiment analysis was used to put large negative weights on swear words and large positive weights on words that were hypothesized to not be in any of the categories via a proprietary swear and hate lexicon. This created a numerical variable that defined the sum of connotations in a comment. Thus if a comment contained a lot of swears or hateful words, sentiment value should predict that the comment was toxic. This was highly effective in that large lists of profane words with negative weights were predictive for many categories.  While these sentiment variables were significant, the possibility that some of the most predictive words were omitted from these lexicons. This represents another area that could be explore dto increase predictive accuracy. 

**Corpus**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
A corpus object is a structure in R that was developed as part of the Quanteda package (Benoit, et al.). The corpus object is optimized for importing plain texts from files in a directory or directly from a vector in R, preprocessing and transforming the texts, and finally exporting them to a document feature matrix.  An additional advantage of corpus utilization in NLP is processing speed.  This type of data object allows for easier filtering, referencing, and manipulation, of text over a more conventional standard data frame. Additionally, this data structure allows other variables to be stored inside using the docvar structure. Ultimately, this is not a tidy structure and is a data frame contained inside of another structure which makes it less intuitive to access them. These objects were necessary in the pipeline of going from vector to document feature matrix but were otherwise not used.

**Document Feature Matrix**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
A document feature matrix is a data structure that records the number  of occurrence of words, tokens, or n-grams in a string of text. This structure comes from the package Quanteda designed by Benoit et al. Each column of these matrices is a specific feature (word/n-gram) of interest and each row is an observation (a comment or sentence). These matrices are usually very large and require some kind of subsetting to remove words/phrases that are not predictive/important and also remove words that occur too frequently such as stopwords. Looking for words that are infrequent but consistently occur in categories of interest is really useful because they are very predictive. Additionally, this matrix structure also allows for the docvar structure mentioned in the corpus section above, however it was not implemented in order to abstain from another unnecessary layer of complication. The generated document feature matrix was appended to our matrix of other variables before using a model to predict on  the six categories of interest. 

**Sparse Matrix**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
A sparse data matrix is a specific type of data containing object inside of R (and maybe other programming languages). This type of matrix is useful because it preserves memory by tracking the locations and values at specific coordinates in an array instead of keeping all the zeros in memory – they are simply omitted. This resulted in a matrix that was only 1.2% of the size of the original matrix. This preserved memory and actually faciliated model building and cross-validation.  There are many implementations of sparse matrices that handle the data in slightly different ways but the specific one was the dgCMatrix class. From the Matrix package documentation this type of matrix is a “Compressed, sparse, column-oriented numeric matri[x]”, the “standard” class for sparse numeric matrices (Maechler, et al.).

**Cross Validation**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
Cross validation is a technique implemented in predictive model development that utilizes re-sampling of the same training and test data multiple times in order to “train” the model over many training and test sets.  This method aids in training the model to make better predictions on new data since the model now avoids the pitfalls of the bias-variance tradeoff (Casella 33-34). The glmnet package (Friedman) was used to cross validate our models since it allowed the use of the doParallel package (Calaway) to parallelize the computation leading to a performance increase. The result was a model that was cross validated 4 times for 50 different lambda values. 

**Logistic Regression**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
Logistic regression is a binary classification method used to estimate the probability of a binary response based on one or more predictor variables (features). It allows evaluation of a variable’s effect on predictions, classification, and outcomes (James et.al, 130).

**Confusion Matrix**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
A confusion matrix is a table that reports:

  - Accuracy: Overall, how often is the classifier correct.
  - Misclassification Rate (error rate): Overall, how often is it wrong.
  - True Positive Rate: When it's actually yes, how often does it predict yes.
  - False Positive Rate: When it's actually no, how often does it predict yes.
  - Specificity: When it's actually no, how often does it predict no.
  - Precision: When it predicts yes, how often is it correct.
  - Prevalence: How often does the yes condition actually occur in our sample.
  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
With this information, we’re able to define a number of metrics that assess how well a model is performing and how much of an improvement has been generated. The main focus in the results section is accuracy because which reports the ratio of correctly predicted observations out of the whole. This is the most widely accepted metric because it is the easily interpretable and a more robust measure of model performance. 

### Coding Procedure

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
Our coding procedure is outlined below and is available for download from our [Github repo](https://github.com/JackWilb97/Toxic-Comment-Classifier):

```{r, warning = FALSE, message = FALSE,}
# Set seed for consistent results
set.seed(1)

# Importing Libraries
library(readr) # Importing and exporting csv files
library(stringi) # Fast string manipulation package (improved version of stringr)
library(quanteda) # Fast tokenizing and dfm creation (see methods)
library(dplyr) # Tidy variable creator and data cleaner
library(caret) # Predictive modeling package, used for confusionMatrix function (see methods) 
library(glmnet) # Predictive modeling package, used for cross validated logistic regression
library(doParallel) # Parallelizing model and dfm building
registerDoParallel(4) # Number of cores to be parallel over
library(tidytext) # Tidy data wrangling, used for stopwords
library(data.table) # Importing swear databases that are friendly with sentimentr
library(sentimentr) # Applying custom sentiments to words of interest

# Importing dataset from Kaggle.com
train = read_csv("train.csv")

# Making the number of rows even, dropping the first observation
train = train[2:159571,]

# Splitting the dataset into a training and a testing set
sample = sample(nrow(train), nrow(train)/2)
test = train[-sample,]
train = train[sample,]
idtrain = 1:nrow(train)
idtest = (nrow(train)+1):(nrow(train) + nrow(test))
traintest = rbind(train[,1:2], test[1:2])

# Using dplyr and stringi to generate new variables related to 
# properties of the strings, names here are descriptive. 
# Output stored in 3 places: train, test, traintest
traintest = rbind(train[,1:2], test[1:2])
traintest = mutate(traintest,
       length = stri_length(comment_text),
       ncap = stri_count_charclass(comment_text, "[A-Z]"),
       nnum = stri_count_charclass(comment_text, "[0-9]"),
       ncap_len = ncap / length,
       nnum_len = nnum / length,
       nexcl = stri_count_fixed(comment_text, "!"),
       nquest = stri_count_fixed(comment_text, "?"),
       npunct = stri_count_charclass(comment_text, "[[:punct:]]"),
       npunct_len = npunct / length,
       nsent = stri_count_boundaries(comment_text, "sentence"),
       nsymb = stri_count_regex(comment_text, "&|@|#|\\$|%|\\*|\\^"),
       nsmile = stri_count_regex(comment_text, "((?::|;|=)(?:-)?(?:\\)|D|P))"),
       nwords = stri_count_words(comment_text)
       )
train = cbind(train, traintest[idtrain,3:ncol(traintest)])
test = cbind(test, traintest[idtest,3:ncol(traintest)])

# Importing swear databases, cleaning them, and making a sentiment key
swears<-fread("swears.csv", sep = ",")
swears<-unique( swears[ , 1:2 ] )
swears$x <- gsub(",","",swears$x)
swears<-unique(swears[,1:2])
mykey.swear<-update_key(swears)

# Importing hate databases, cleaning them, and making a sentiment key.
# More code than before because there was more to clean
hatewords<-fread("hatebase_dict1.csv",sep=",")
hatewords<-unique(hatewords[,1:2])
hatewords$words<-gsub(",","",hatewords$words)
hatewords$words<-gsub("'"," ",hatewords$words)
hatewords$words<-tolower(hatewords$words)
colnames(hatewords)[1] <- "x"
colnames(hatewords)[2]<-"y"
hatewords<-unique(hatewords[,1:2])
key.hatewords <- update_key(hatewords)

# Removing stop words and some cleaning of the stopwords
stopWords = c(stop_words$word, "of", "or", "on")
stopWords=stopWords[!(stopWords %in% c("new","used","small","large","thanks","greetings","works"))]

# Creating new data frames to train run sentiment analysis
cmmnt = train$comment_text
cmmnt = unlist(cmmnt)[!(unlist(cmmnt) %in% stopWords)]
cmmnt = stri_replace_all_charclass(cmmnt, "[^[:alnum:]]", " ")
id = rep(1:nrow(train))
cmmnt_df = data.frame(id = id, comments = cmmnt)
cmmnt_df$comments = as.character(cmmnt_df$comments)

cmmnt.t = test$comment_text
cmmnt.t = unlist(cmmnt.t)[!(unlist(cmmnt.t) %in% stopWords)]
cmmnt.t = stri_replace_all_charclass(cmmnt.t, "[^[:alnum:]]", " ")
id = rep(1:nrow(test))
cmmnt_df.t = data.frame(id = id, comments = cmmnt.t)
cmmnt_df.t$comments = as.character(cmmnt_df.t$comments)

# Running swear sentiment analysis
swear.sent.train<-sentiment_by(cmmnt_df$comments,list(id), polarity_dt=mykey.swear)
swear.sent.test<-sentiment_by(cmmnt_df.t$comments,list(id),polarity_dt=mykey.swear)
colnames(swear.sent.test)[4] <- "swear_sent"
colnames(swear.sent.train)[4]<-"swear_sent"
train<-cbind(train,swear.sent.train[,4])
test<-cbind(test,swear.sent.test[,4])

# Running regular sentiment analysis with afinn lexicon
pos.sent<-sentiment_by(cmmnt_df$comments, list(id), lexicon="afinn")
pos.sent.t<-sentiment_by(cmmnt_df$comments, list(id), lexicon="afinn")
colnames(pos.sent)[4] <- "afinn"
colnames(pos.sent.t)[4]<-"afinn"
train<-cbind(train,pos.sent[,4])
test<-cbind(test,pos.sent.t[,4])


# Running corpus from quanteda, the first step to creating the document feature matrix
ttcorp = corpus(traintest$comment_text)
ttdfm = dfm(ttcorp, remove = stopwords("english"))
ttdfm = dfm_trim(ttdfm, min_docfreq = 150, max_docfreq = 100000)
ttdfm = dfm_sort(ttdfm)

# Converting the corpus to a sparse matrix
sumttdfm = summary(ttdfm)
sparsettdfm = sparseMatrix(i = sumttdfm$i, j = sumttdfm$j, x = sumttdfm$x)

# Converting train and test to sparse and then binding on document feature matrix to each
sparsetrain = Matrix(as.matrix(train[3:23]), sparse = TRUE)
sparsetest = Matrix(as.matrix(test[3:23]), sparse = TRUE)
train = cbind(sparsetrain, sparsettdfm[1:nrow(train),])
test = cbind(sparsetest, sparsettdfm[(nrow(train)+1):(nrow(train) + nrow(test)),])
rm(sparsetrain, sparsetest)

# Running toxic model
toxicglm = cv.glmnet(train[,8:dim(train)[2]], 
                     factor(train[,1]), 
                     alpha = 0, 
                     family = "binomial", 
                     type.measure = "auc",
                     parallel = T,
                     nfolds = 40,
                     standardize = T,
                     nlambda = 50)
toxicpreds = predict(toxicglm, newx = test[,8:dim(test)[2]], type = "response", s = "lambda.min")
toxicpreds = ifelse(toxicpreds > 0.27, 1 ,0)
toxicconf = confusionMatrix(factor(toxicpreds), factor(test[,1]))

# Running severe toxic model
severe_toxicglm = cv.glmnet(train[,8:dim(train)[2]], 
                     factor(train[,2]), 
                     alpha = 0, 
                     family = "binomial", 
                     type.measure = "auc",
                     parallel = T,
                     nfolds = 4,
                     standardize = T,
                     nlambda = 50)
severe_toxicpreds = predict(severe_toxicglm, newx = test[,8:dim(test)[2]], type = "response", s = "lambda.min")
severe_toxicpreds = ifelse(severe_toxicpreds > 0.02, 1 ,0)
severe_toxicconf = confusionMatrix(factor(severe_toxicpreds), factor(test[,2]))

# Running obscene model
obsceneglm = cv.glmnet(train[,8:dim(train)[2]], 
                     factor(train[,3]), 
                     alpha = 0, 
                     family = "binomial", 
                     type.measure = "auc",
                     parallel = T,
                     nfolds = 4,
                     standardize = T,
                     nlambda = 50)
obscenepreds = predict(obsceneglm, newx = test[,8:dim(test)[2]], type = "response", s = "lambda.min")
obscenepreds = ifelse(obscenepreds > 0.22, 1 ,0)
obsceneconf = confusionMatrix(factor(obscenepreds), factor(test[,3]))

# Running threat model
threatglm = cv.glmnet(train[,8:dim(train)[2]], 
                     factor(train[,4]), 
                     alpha = 0, 
                     family = "binomial", 
                     type.measure = "auc",
                     parallel = T,
                     nfolds = 4,
                     standardize = T,
                     nlambda = 50)
threatpreds = predict(threatglm, newx = test[,8:dim(test)[2]], type = "response", s = "lambda.min")
threatpreds = ifelse(threatpreds > 1, 1 ,0)
threatconf = confusionMatrix(factor(threatpreds), factor(test[,4]))

# Running insult model
insultglm = cv.glmnet(train[,8:dim(train)[2]], 
                     factor(train[,5]), 
                     alpha = 0, 
                     family = "binomial", 
                     type.measure = "auc",
                     parallel = T,
                     nfolds = 4,
                     standardize = T,
                     nlambda = 50)
insultpreds = predict(insultglm, newx = test[,8:dim(test)[2]], type = "response", s = "lambda.min")
insultpreds = ifelse(insultpreds > 0.21, 1 ,0)
insultconf = confusionMatrix(factor(insultpreds), factor(test[,5]))

# Running identity hate model model
identity_hateglm = cv.glmnet(train[,8:dim(train)[2]], 
                     factor(train[,6]), 
                     alpha = 0, 
                     family = "binomial", 
                     type.measure = "auc",
                     parallel = T,
                     nfolds = 4,
                     standardize = T,
                     nlambda = 50)
identity_hatepreds = predict(identity_hateglm, newx = test[,8:dim(test)[2]], type = "response", s = "lambda.min")
identity_hatepreds = ifelse(identity_hatepreds > 1, 1 ,0)
identity_hateconf = confusionMatrix(factor(identity_hatepreds), factor(test[,6]))
```

### Results

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
After creating the variables explained above, these predictors were used with a cross validated logistic regression to classify comments as Toxic, Severe_Toxic, Obscene, Threat, Insult, and Identity Hate. The results are evaluated as percent of accurate classifications made and p-values are given as an improvement from the no-information rate.

> Toxic: `r unname((toxicconf$overall[1]-toxicconf$overall[5])/(1-toxicconf$overall[5]))*100` % increase 
  (p < 0.001, CI = (`r toxicconf$overall[3]` , `r toxicconf$overall[4]`), NIR = `r toxicconf$overall[5]`)

> Severe_Toxic: `r unname((severe_toxicconf$overall[1]-severe_toxicconf$overall[5])/(1-severe_toxicconf$overall[5]))*100` % increase
  (p = `r severe_toxicconf$overall[6]`, CI = (`r severe_toxicconf$overall[3]` , `r severe_toxicconf$overall[4]`), NIR = `r severe_toxicconf$overall[5]`) 

> Obscene: `r unname((obsceneconf$overall[1]-obsceneconf$overall[5])/(1-obsceneconf$overall[5]))*100` % increase
  (p < 0.001, CI = (`r obsceneconf$overall[3]` , `r obsceneconf$overall[4]`), NIR = `r obsceneconf$overall[5]`)

> Threat: `r unname((threatconf$overall[1]-threatconf$overall[5])/(1-threatconf$overall[5]))*100` % increase
  (p = `r threatconf$overall[6]`, CI = (`r threatconf$overall[3]` , `r threatconf$overall[4]`), NIR = `r threatconf$overall[5]`)

> Insult: `r unname((insultconf$overall[1]-insultconf$overall[5])/(1-insultconf$overall[5]))*100` % increase
  (p < 0.001, CI = (`r insultconf$overall[3]` , `r insultconf$overall[4]`), NIR = `r insultconf$overall[5]`)

> Hate: `r unname((identity_hateconf$overall[1]-identity_hateconf$overall[5])/(1-identity_hateconf$overall[5]))*100` % increase
  (p = `r identity_hateconf$overall[6]`, CI = (`r identity_hateconf$overall[3]` , `r identity_hateconf$overall[4]`), NIR = `r identity_hateconf$overall[5]`)

### Discussion

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
The results indicate that statistically significant classifications can be made by analysis of comment structure and sentiment analysis.  Specifically, we were able to make statistically significant classifications for toxic, obscene, and insulting comments. The combination of tokens and sentiment analysis was improvement over each method independently and substantiates intuition that Natural Language Processing is multi-facetted. We recognize that further relationships between structure, sentiment, and other factors, could play an important role in accurate comment classifications but that we were limited by time and computing power when attempting to accurately classify comments. If we were able to repeat this case study we’d devote more time to n-grams and to exploring other variables that could be predictive.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
The lack of accurate predictions of severe toxic, threat, and identity hate comments represents another interesting question. Since the no information rate for these categories was so high, it was almost impossible to draw out any meaningful information that would lead to better classifications. The target was simply too small. It would be interesting to run this analysis again with these comment types representing a larger portion of the data because our methods should be robust enough to make predictions on these categories. Additionally, further model analysis is needed to understand why useful information couldn’t be parsed out since almost every word from the comments was considered via the document feature matrix- perhaps n-gram and interactions between variables would yield better accuracy.

### Conclusion

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
This study combined techniques commonly used for text inference in order to predict classifications.  The ability to draw meaningful inference from vasts amount of text data is inextricably connected to being able to make predictions and classify text.  This study explored a method of examining text structure and word sentiment to classify comments  producing insight from large amounts of text data. Ultimately we are happy with the improvements made despite the previously mentioned limitations and are motivated to explore this field further. As a first introduction to NLP and its’ associated predictive modeling, this was a great experience that let us get hands on with data and dig into new methodology. 

### Acknowledgements

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
This study was facilitated by our faculty advisor Sean Raleigh and was supported by Westminster College. We’d also like to thank Kaggle for hosting this data and creating the competition that this data was drawn from. Additionally, we’d be remiss to not acknowledge Wikipedia’s role in providing the data and the incentives on Kaggle.

### Works Cited

Benoit, Kenneth, et al.  “Quanteda.” Quanteda Package | R Documentation, CRAN, 15 Apr. 2018

Brownlee, Jason. 22 Sept. 2017, Machine Learning Mastery, machinelearningmastery.com/natural-language-processing/. 

Bryant, Charles W. “How Did Language Evolve?” HowStuffWorks Science, HowStuffWorks, 8 Mar. 2018, science.howstuffworks.com/life/evolution/language-evolve.htm.

Calaway, Rich, et al.  “doParallel.” doParallel Package | R Documentation, CRAN, 28 Sep. 2017

Carpenter, Rollo. “CleverBot.” cleverbot.com

Chowdhury, Gobinda G. “Natural Language Processing.” Annual Review of Information Science and Technology, Wiley-Blackwell, 31 Jan. 2005, onlinelibrary.wiley.com/doi/full/10.1002/aris.1440370103.

Friedman, Jerome, et al. “Glmnet.” Glmnet Package | R Documentation, CRAN, 02 Apr. 2018

Hutchins, John. “The History Of Machine Translation.” Jan 2014.mt-archive.info/10/Hutchins-2014.pdf

James, Gareth, et al. An Introduction to Statistical Learning: with Applications in R. Springer, 2017. 

Maechler, Martin, et al. “Matrix.” Matrix Package | R Documentation, CRAN, 11 Nov. 2017

Silge, Julia & Robinson, David. “Tidy Mining with R, A Tidy Approach”.

“The Exponential Growth of Data.” Edited by Editorial Team, InsideBIGDATA, 17 Apr. 2018, insidebigdata.com/2017/02/16/the-exponential-growth-of-data/.

Wickham, Hadley, et al. “Readr.” Readr Package | R Documentation, CRAN, 16 May. 2017

Winograd, Terry. SHRDLU, Stanford, hci.stanford.edu/winograd/shrdlu/. 

