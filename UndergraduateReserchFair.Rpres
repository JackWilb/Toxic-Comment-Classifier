Natural Language Processing In R: A Wikipedia Case Study
========================================================
author: Jack Wilburn and Rob Squire
font-family: Georgia
date: 20th April 2018
autosize: true

```{r setup, include=FALSE}
opts_chunk$set(cache=TRUE)
```

Motivation
========================================================

* Text Analysis
* Develop Techniques
  - Data Aggregation/Cleaning
  - Modeling
  - Reporting

Objectives
========================================================

* Natural Language Processing (NLP)
  - Body Of Text
  - Useful Variables
* Predictive Modeling
  - Explantory Variables
  - 6 Response Variables
  - Inference
  
Methods 
========================================================

* Explore Raw Data

* Clean Data

* Create Variables

* Split data
  - Training/Test
  
Methods 
========================================================  

* Implement models 
  - Cross Validated Logistic Regression

* Assess Accuracy
  - Confusion Matrix

Libraries
========================================================

```{r, include = FALSE}
set.seed(1)
library(readr)
library(stringi)
library(quanteda)
library(dplyr)
library(caret)
library(glmnet)
library(doParallel)
registerDoParallel(4)
library(tidytext)
library(data.table)
library(sentimentr)
```

* readr
* stringi
* quanteda
* dplyr
* caret

***

* glmnet
* doParallel
* tidytext
* data.table
* sentimentr

Exploratory Data Analysis
========================================================

```{r,include=FALSE}
library(tidyverse)
train = read_csv("train.csv")
```

* Structure Of The Data
```{r, echo=FALSE}
head(as.data.frame(train[100,]), 1)
```

Exploratory Data Analysis
========================================================

```{r, include=FALSE}
table(train$toxic)
table(train$obscene)
table(train$threat)
table(train$insult)
table(train$identity_hate)
```

* Toxic: `r (15294/159571)*100` %.

* Severe_toxic: `r (1595/159571)*100 ` %.

* Obscene: `r (8449/159571)*100 ` %.

* Threat: `r (478/159571)*100 ` %.

* Insult: `r (7877/159571)*100 ` %.

* Identity Hate: `r (1405/159571)*100 ` %.

Tidy Text Word Analysis
========================================================

```{r, warning=FALSE, include=FALSE}
traincorpus = corpus(train$comment_text)
docvars(traincorpus, "id") = 1:nrow(train)
docvars(traincorpus, "toxic") = train$toxic
docvars(traincorpus, "severe_toxic") = train$severe_toxic
docvars(traincorpus, "obscene") = train$obscene
docvars(traincorpus, "threat") = train$threat
docvars(traincorpus, "insult") = train$insult
docvars(traincorpus, "identity_hate") = train$identity_hate
docvars(traincorpus, "ntokens") = ntoken(traincorpus, remove_punct = TRUE)

traindfm2 = dfm(traincorpus, remove = stopwords("english"), stem = TRUE, remove_punct = TRUE)
toxicdfm2 = dfm_subset(traindfm2, toxic == 1, remove = stopwords("english"))
toxicfeatures2 = topfeatures(toxicdfm2, 10)
toxicwords2 = names(toxicfeatures2)

severe_toxicdfm2 = dfm_subset(traindfm2, severe_toxic == 1,remove = stopwords("english"))
severe_toxicfeatures2 = topfeatures(severe_toxicdfm2, 10)
severe_toxicwords2 = names(severe_toxicfeatures2)

obscenedfm2 = dfm_subset(traindfm2, obscene == 1,remove = stopwords("english"))
obscenefeatures2 = topfeatures(obscenedfm2, 10)
obscenewords2 = names(obscenefeatures2)

threatdfm2 = dfm_subset(traindfm2, threat == 1,remove = stopwords("english"))
threatfeatures2 = topfeatures(threatdfm2, 10)
threatwords2 = names(threatfeatures2)

insultdfm2 = dfm_subset(traindfm2, insult == 1,remove = stopwords("english"))
insultfeatures2 = topfeatures(insultdfm2, 10)
insultwords2 = names(insultfeatures2)

identity_hatedfm2 = dfm_subset(traindfm2, identity_hate == 1,remove = stopwords("english"))
identity_hatefeatures2 = topfeatures(identity_hatedfm2, 10)
identity_hatewords2 = names(identity_hatefeatures2)

a = data.frame(term = c("f**k","ni**er","a*s","sex","hate","go","c*nt","like","suck","moron"), freq = unname(toxicfeatures2)) %>%
    ggplot(aes(x = reorder(term, freq), y = freq)) +
    geom_bar(stat = 'identity', fill = 'orangered2') + axis.text.y = theme_text(size=35)+
    labs(x = '', y = 'Frequency', title = 'Most common Toxic Words') + 
    coord_flip()

b = data.frame(term = c("f**k","a*s","suck","u","go","bastard","c*nt","athiest","fire","pro.assad.hannibal911you'r"), freq = unname(severe_toxicfeatures2)) %>%
    ggplot(aes(x = reorder(term, freq), y = freq)) +
    geom_bar(stat = 'identity', fill = 'orangered2') + theme(text=element_text(size=70))+
    labs(x = '', y = 'Frequency', title = 'Most common Severe_toxic Words ') + 
    coord_flip() 

c = data.frame(term = c("f**k","ni**er","a*s","c*nt","suck","go","u","bollock","d*ckhead","like"), freq = unname(obscenefeatures2)) %>%
    ggplot(aes(x = reorder(term, freq), y = freq)) + theme(text=element_text(size=70))+
    geom_bar(stat = 'identity', fill = 'orangered2') + 
    labs(x = '', y = 'Frequency', title = 'Most common Obscene Words') + 
    coord_flip() 

d = data.frame(term = c("a*s","will","supertr0l","die","live","pathet(ic)","forev(er)","fool","respect","f**k"), freq = unname(threatfeatures2)) %>%
    ggplot(aes(x = reorder(term, freq), y = freq)) + 
    geom_bar(stat = 'identity', fill = 'orangered2') +theme(text=element_text(size=70))+ 
    labs(x = '', y = 'Frequency', title = 'Most common Threat Word') + 
    coord_flip() 

e = data.frame(term = c("f**k","ni**er","a*s","hate","c*nt","go","moron","hi","u","suck"), freq = unname(insultfeatures2)) %>%
    ggplot(aes(x = reorder(term, freq), y = freq)) + 
    geom_bar(stat = 'identity', fill = 'orangered2') + theme(text=element_text(size=70))+
    labs(x = '', y = 'Frequency', title = 'Most common Insult Word') + 
    coord_flip() 


f = data.frame(term = c("ni**er","c*nt","tommy2010","licker","fan-1967","like","romney","mitt","homo","f**k"), freq = unname(identity_hatefeatures2)) %>%
    ggplot(aes(x = reorder(term, freq), y = freq)) + theme(text=element_text(size=70))+
    geom_bar(stat = 'identity', fill = 'orangered2') + 
    labs(x = '', y = 'Frequency', title = 'Most common Identity_hate Word') + 
    coord_flip() 
```

```{r, echo = FALSE, fig.width = 18, fig.height = 10}
gridExtra::grid.arrange(a,b, ncol = 2)
```

Tidy Text Word Analysis
========================================================

```{r, echo = FALSE, fig.width = 18, fig.height = 10}
gridExtra::grid.arrange(c,d, ncol = 2)
```

Tidy Text Word Analysis
========================================================

```{r, echo = FALSE, fig.width = 18, fig.height = 10}
gridExtra::grid.arrange(e,f, ncol = 2)
```


These word frequency visuals gives us an idea of words associated with certain classifications.  We can add counts of these words into our variable creation to make better predictions.



PCA
========================================================

```{r, echo=FALSE, fig.width=18, fig.height=10}
train_num<-select_if(train, is.numeric)
train_num<-na.omit(train_num)
train_pca<- prcomp(train_num, scale=TRUE)
biplot(train_pca, xlabs=rep("",159571))
```
This loosely corresponds to correlation of comments.  It behaves as we'd expect.


Variables Created
========================================================

*Feature Count 
  -punctuation
  -capital letters
  -smileys
  
*Sentiment analysis
  -Standard Lexicon/ library - AFINN
  - Custom lexicon
    -Swear
    -Hate
    
- Sparse Document Feature Matrix
  -counts of every possible "feature"
    -words, punt, multiple words
    -4000 features.
   


Results
========================================================

```{r, include = FALSE}
toxicglm = cv.glmnet(train[,8:dim(train)[2]], 
                     factor(train[,1]), 
                     alpha = 0, 
                     family = "binomial", 
                     type.measure = "auc",
                     parallel = T,
                     nfolds = 40,
                     standardize = T,
                     nlambda = 50)
toxicpreds = predict(toxicglm, newx = test[,8:dim(test)[2]], type = "response", s = "lambda.min")
toxicpreds = ifelse(toxicpreds > 0.27, 1 ,0)
toxicconf = confusionMatrix(factor(toxicpreds), factor(test[,1]))

severe_toxicglm = cv.glmnet(train[,8:dim(train)[2]], 
                     factor(train[,2]), 
                     alpha = 0, 
                     family = "binomial", 
                     type.measure = "auc",
                     parallel = T,
                     nfolds = 4,
                     standardize = T,
                     nlambda = 50)
severe_toxicpreds = predict(severe_toxicglm, newx = test[,8:dim(test)[2]], type = "response", s = "lambda.min")
severe_toxicpreds = ifelse(severe_toxicpreds > 0.02, 1 ,0)
severe_toxicconf = confusionMatrix(factor(severe_toxicpreds), factor(test[,2]))

obsceneglm = cv.glmnet(train[,8:dim(train)[2]], 
                     factor(train[,3]), 
                     alpha = 0, 
                     family = "binomial", 
                     type.measure = "auc",
                     parallel = T,
                     nfolds = 4,
                     standardize = T,
                     nlambda = 50)
obscenepreds = predict(obsceneglm, newx = test[,8:dim(test)[2]], type = "response", s = "lambda.min")
obscenepreds = ifelse(obscenepreds > 0.22, 1 ,0)
obsceneconf = confusionMatrix(factor(obscenepreds), factor(test[,3]))

threatglm = cv.glmnet(train[,8:dim(train)[2]], 
                     factor(train[,4]), 
                     alpha = 0, 
                     family = "binomial", 
                     type.measure = "auc",
                     parallel = T,
                     nfolds = 4,
                     standardize = T,
                     nlambda = 50)
threatpreds = predict(threatglm, newx = test[,8:dim(test)[2]], type = "response", s = "lambda.min")
threatpreds = ifelse(threatpreds > 1, 1 ,0)
threatconf = confusionMatrix(factor(threatpreds), factor(test[,4]))

insultglm = cv.glmnet(train[,8:dim(train)[2]], 
                     factor(train[,5]), 
                     alpha = 0, 
                     family = "binomial", 
                     type.measure = "auc",
                     parallel = T,
                     nfolds = 4,
                     standardize = T,
                     nlambda = 50)
insultpreds = predict(insultglm, newx = test[,8:dim(test)[2]], type = "response", s = "lambda.min")
insultpreds = ifelse(insultpreds > 0.21, 1 ,0)
insultconf = confusionMatrix(factor(insultpreds), factor(test[,5]))

identity_hateglm = cv.glmnet(train[,8:dim(train)[2]], 
                     factor(train[,6]), 
                     alpha = 0, 
                     family = "binomial", 
                     type.measure = "auc",
                     parallel = T,
                     nfolds = 4,
                     standardize = T,
                     nlambda = 50)
identity_hatepreds = predict(identity_hateglm, newx = test[,8:dim(test)[2]], type = "response", s = "lambda.min")
identity_hatepreds = ifelse(identity_hatepreds > 1, 1 ,0)
identity_hateconf = confusionMatrix(factor(identity_hatepreds), factor(test[,6]))
```


Toxic = `r unname((toxicconf$overall[1]-toxicconf$overall[5])/(1-toxicconf$overall[5]))*100`
Severe_Toxic = `r unname((severe_toxicconf$overall[1]-severe_toxicconf$overall[5])/(1-severe_toxicconf$overall[5]))*100`
Obscene = `r unname((obsceneconf$overall[1]-obsceneconf$overall[5])/(1-obsceneconf$overall[5]))*100`
Threat = `r unname((threatconf$overall[1]-threatconf$overall[5])/(1-threatconf$overall[5]))*100`
Insult = `r unname((insultconf$overall[1]-insultconf$overall[5])/(1-insultconf$overall[5]))*100`
Identity_Hate = `r unname((identity_hateconf$overall[1]-identity_hateconf$overall[5])/(1-identity_hateconf$overall[5]))*100`

Discussion
========================================================

Satistically signficant Classifications:

  -Toxic
  -Obscene
  -Insult
  
No Improvement:

  -Severe_toxic
  -Threat
  -Identity_hate
  
Significant Variables:

  -Sentiment Analysis
  -Document Feature Matrix


Limitations
========================================================

* Data Size:
  - Small, number of certain categories (0.3% Threat)
  
  - Machine Learning limited

* Computing power:
  
  - N-grams
  
  - Feature Combinations
  
  - Unkown Variables


Acknowledgements
========================================================

http://www.bannedwordlist.com/lists/swearWords.txt

https://www.cs.cmu.edu/~biglou/resources/bad-words.txt

https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/lexicons

https://www.tidytextmining.com/tidytext.html

https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge



