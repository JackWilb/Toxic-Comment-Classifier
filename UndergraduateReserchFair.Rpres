Natural Language Processing In R: A Wikipedia Case Study
========================================================
author: Jack Wilburn and Rob Squire
font-family: Georgia
date: 20th April 2018
autosize: true

Motivation
========================================================

* Kaggle
  - $50,000
* Skills
  - New
  - Old

Objectives
========================================================

* NLP
  - Body Of Text
  - Useful Variables
  - Example
* Predictive Modeling
  - Explantory Variables
  - Response Variable

Exploratory Data Analysis
========================================================

```{r, echo=FALSE}
library(tidyverse)
train = read_csv("train.csv")
head(train$comment_text, 2)
names(train)
```

Structure
========================================================

```{r, echo=FALSE}
train[1,]
```




Data Exploration
========================================================
##Libraries
```{r, message = FALSE, warning = FALSE}
library(readr)
library(stringi)
library(quanteda)
library(dplyr)
library(caret)
library(glmnet)
library(doParallel)
library(tidytext)
library(data.table)
library(sentimentr)
```

##Exploratory
Now that we have some additional variables lets explore the data a bit.  
First lets count what we have.
```{r}
tally(train$toxic)
tally(train$severe_toxic)
tally(train$obscene)
tally(train$threat)
tally(train$insult)
tally(train$identity_hate)
```
The tally command tells us exactly how many of each category there are so we know what we are looking at.  Initial observations show that some categories like "threat" are quite small which may make classifying "threat" comments difficult. Let's take a look at the most common words from each category using tidy text.


##Tidy Text Word Analysis
```{r, warning=FALSE}
traincorpus = corpus(train$comment_text)
docvars(traincorpus, "id") = 1:nrow(train)
docvars(traincorpus, "toxic") = train$toxic
docvars(traincorpus, "severe_toxic") = train$severe_toxic
docvars(traincorpus, "obscene") = train$obscene
docvars(traincorpus, "threat") = train$threat
docvars(traincorpus, "insult") = train$insult
docvars(traincorpus, "identity_hate") = train$identity_hate
docvars(traincorpus, "ntokens") = ntoken(traincorpus, remove_punct = TRUE)





traindfm2 = dfm(traincorpus, remove = stopwords("english"), stem = TRUE, remove_punct = TRUE)


toxicdfm2 = dfm_subset(traindfm2, toxic == 1, remove = stopwords("english"))
toxicfeatures2 = topfeatures(toxicdfm2, 10)
toxicwords2 = names(toxicfeatures2)

severe_toxicdfm2 = dfm_subset(traindfm2, severe_toxic == 1,remove = stopwords("english"))
severe_toxicfeatures2 = topfeatures(severe_toxicdfm2, 10)
severe_toxicwords2 = names(severe_toxicfeatures2)

obscenedfm2 = dfm_subset(traindfm2, obscene == 1,remove = stopwords("english"))
obscenefeatures2 = topfeatures(obscenedfm2, 10)
obscenewords2 = names(obscenefeatures2)

threatdfm2 = dfm_subset(traindfm2, threat == 1,remove = stopwords("english"))
threatfeatures2 = topfeatures(threatdfm2, 10)
threatwords2 = names(threatfeatures2)

insultdfm2 = dfm_subset(traindfm2, insult == 1,remove = stopwords("english"))
insultfeatures2 = topfeatures(insultdfm2, 10)
insultwords2 = names(insultfeatures2)

identity_hatedfm2 = dfm_subset(traindfm2, identity_hate == 1,remove = stopwords("english"))
identity_hatefeatures2 = topfeatures(identity_hatedfm2, 10)
identity_hatewords2 = names(identity_hatefeatures2)



data.frame(term = c("f**k","ni**er","a*s","sex","hate","go","c*nt","like","suck","moron"), freq = unname(toxicfeatures2)) %>%
    ggplot(aes(x = reorder(term, freq), y = freq)) + 
    geom_bar(stat = 'identity', fill = 'orangered2') + 
    labs(x = '', y = 'Frequency', title = 'Most common Toxic Words') + 
    coord_flip() 

data.frame(term = c("f**k","a*s","suck","u","go","bastard","c*nt","athiest","fire","pro.assad.hannibal911you'r"), freq = unname(severe_toxicfeatures2)) %>%
    ggplot(aes(x = reorder(term, freq), y = freq)) + 
    geom_bar(stat = 'identity', fill = 'orangered2') + 
    labs(x = '', y = 'Frequency', title = 'Most common Severe_toxic Words ') + 
    coord_flip() 

data.frame(term = c("f**k","ni**er","a*s","c*nt","suck","go","u","bollock","d*ckhead","like"), freq = unname(obscenefeatures2)) %>%
    ggplot(aes(x = reorder(term, freq), y = freq)) + 
    geom_bar(stat = 'identity', fill = 'orangered2') + 
    labs(x = '', y = 'Frequency', title = 'Most common Obscene Words') + 
    coord_flip() 

data.frame(term = c("a*s","will","supertr0l","die","live","pathet(ic)","forev(er)","fool","respect","f**k"), freq = unname(threatfeatures2)) %>%
    ggplot(aes(x = reorder(term, freq), y = freq)) + 
    geom_bar(stat = 'identity', fill = 'orangered2') + 
    labs(x = '', y = 'Frequency', title = 'Most common Threat Word') + 
    coord_flip() 

data.frame(term = c("f**k","ni**er","a*s","hate","c*nt","go","moron","hi","u","suck"), freq = unname(insultfeatures2)) %>%
    ggplot(aes(x = reorder(term, freq), y = freq)) + 
    geom_bar(stat = 'identity', fill = 'orangered2') + 
    labs(x = '', y = 'Frequency', title = 'Most common Insult Word') + 
    coord_flip() 


data.frame(term = c("ni**er","c*nt","tommy2010","licker","fan-1967","like","romney","mitt","homo","f**k"), freq = unname(identity_hatefeatures2)) %>%
    ggplot(aes(x = reorder(term, freq), y = freq)) + 
    geom_bar(stat = 'identity', fill = 'orangered2') + 
    labs(x = '', y = 'Frequency', title = 'Most common Identity_hate Word') + 
    coord_flip() 
```
These word frequency visuals gives us an idea of words associated with certain classifications.  We can add counts of these words into our variable creation to make better predictions.

##PCA
```{r}
train_num<-select_if(train, is.numeric)
train_num<-na.omit(train_num)


train_pca<- prcomp(train_num, scale=TRUE)
train_pca_tidy<- tidy(train_pca, matrix = "variables")
head(train_pca_tidy, n=20)
train_pca_tidy_wide<- spread(train_pca_tidy, key=PC, value=value)

head(cars_pca_tidy_wide)
train_pca_aug<-augment(train_pca)
head(train_pca_aug)
```
```{r fig.width=14, fig.height=14}
biplot(train_pca, xlabs=rep("",15000))
```


