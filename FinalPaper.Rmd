---
title: "Final Paper"
author: "Jack Wilburn and Rob Squire"
date: "4/30/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(cache=TRUE)
```

### Abstract

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The purpose of this semester long project was to learn a new skill in R: Natural Language Processing (NLP) and, ultimately, we were successful. The impetus for this project came from Kaggle.com, a site that hosts data science related challenges, in the form of a competition that asked us to classify certain Wikipedia submission comments into 6 categories (toxic, severe toxic, obscene, insult, threat, and identity hate). Our objectives were to understand what NLP is and to create a cross validated logistic regression model that would accurately classify the comment text based on these categories. We were successful in both endeavours and were able to predict comment categories with as much as 31.29% improvement over the no-information rate, significant for us but not the best entry in the competition. This newly gained knowledge will help us when formulating other NLP based predictive models and the research methodology we used will help us learn other techniques for future model building in any setting. In all, this project was a great first foray into NLP and will be a great stepping off point for many other projects.

**Keywords: Cross Validation, Document Feature Matrix, Logistic Regression, Natural Language Processing (NLP), Sentiment Analysis, Sparse Matrix, Text Mining**

### Introduction

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  The spoken and written language of humans, has been the predominant method for conveying ideas, stories, thoughts, etc. for milenia. In fact, “language first appeared among Homo sapiens somewhere between 30,000 and 100,000 years ago” (Bryant), and is still one of a few of communication. These verbal and written words necessitate reading, or listening, and then an accurate comprehension of the words; however, the creation of ideas has far outpaced our ability to interpret in a timely fashion. As a consequence, there are large bodies of knowledge that lie unread or unheard and thus that knowledge has not been co-opted into society. While concepts have been able to evolve based on preceding work, the methods of interpretation remain limited by human comprehension.
With the onset of computers, we are now on the precipice of a new method for language comprehension at speeds that far outpace human ability.  Developing methods for computers to interpret language presents the ability extract key bits of information from vast amounts of language.  
 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; With the increasing global access to technology, data is being created and consumed at an ever increasing rate (insideBIGDATA), requiring development of analysis techniques that can effectively process language, written or spoken.  The ability to condense a body of text from: Twitter tweets, books, internet comments, websites, etc, is vital to drawing meaningful information from vasts amount of text. We can achieve this condensation in many ways; for example, we could use a dimension reduction or, more commonly, a feature selection. 
After this condensation we can apply an algorithm that will allow us to predict certain facets of interest.

### Methods

**Natural Language Processing (NLP)**
    
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; At a basic level, natural language processing is a method through which researchers can develop understanding about large (or small) sets of text data by considering how humans interact and how they present themselves on a computer. The goal of NLP is to breakdown strings of text such as comments, product reviews, or even books to extract informative, or even predictive, variables from words that don’t inherently have meaning. (Chowdhury, 51). NLP is colloquially referred to as text mining and is the backbone of this project. We used NLP extensively to clean, process, and represent the text strings in the Wikipedia comments as new variables that may have predictive power. The variables we created are included later in this section and they allowed us to predict whether or not comments would be some type of offensive. 
    
**Variable Creation**
    
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Variable creation was the most important step in this project. Going from plain text to numerical/categorical variables that represent important information allowed us to predict was the hurdle that we had to leap over and comprised the most of our time. In all we created about 4000 variables that might help us explain what was happening.

**Tokens**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A token, put simply, is a word inside a string of text. The only important thing to note is that tokens only count as one word even if they are in a string multiple times. Take for example the string “I love love data.” In this string, there would be only 3 tokens, “I”, “love”, and “data.” This is the convention of the tidy text package and the function tokenizer. Tokens are useful in understanding which words are present and how many times they occur in a vector of strings but not for understanding total occurrences in either one string or the entire string vector.

**N-grams**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; An n-gram is similar to a token but is a string comprising of n words. For example, you might notice that the words “love” and “you” occur consecutively so instead of searching for each word as a token, you might instead search for the 2-gram “love you”. This might help you dial in a more specific sentiment for groupings of words as opposed to if you searched for each word independently. 

**Sentiment Analysis**
    
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sentiment analysis consists of assigning values to words, tokens, or n-grams in order to classify/interpret a specific body of text. In our case we used sentiment analysis to put large negative weights on swear words and large positive weights on words that we believed would be would not be in any of the categories we were predicting.

**Document Feature Matrix**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A document feature matrix is a relatively simple structure that records the number  of occurrence of words, tokens, or n-grams. These matrices are usually very large and require some kind of sub-setting to remove words/phrases that are not predictive and also removing words that occur too frequently. Looking for words that are infrequent but consistently occur in categories of interest is really useful because they are very predictive.

**Sparse Matrix**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A sparse data matrix is a specific type of data containing object inside of R (and maybe other programming languages). This type of matrix is useful because it preserves memory by tracking the locations and values at specific coordinates in an array instead of keeping all the zeros in memory – they are simply omitted. This meant in our case that the matrix that we held our data in was able to be stored in memory and was actually only SOMETHING% of the size of the original matrix. This meant that we were able to preserve memory and actually allowed us to build our model and cross validate it. There are many implementations of sparse matrices that handle the data in slightly different ways but the one we used specifically was the 


**Cross Validation**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Cross validation is a technique implemented in predictive model development that utilizes re-sampling of the same training and test data multiple times in order to “train” the model over many training and test sets.  This method aids in training the model to make better predictions on new data. 

**Logistic Regression**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Logistic regression is a binary classification method used to estimate the probability of a binary response based on one or more predictor variables (features). It allows evaluation of a variable’s effect on predictions, classification, and outcomes. 

**Confusion Matrix**
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 	A confusion matrix is a table that reports:
-Accuracy: Overall, how often is the classifier correct.
-Misclassification Rate (error rate): Overall, how often is it wrong.
-True Positive Rate: When it's actually yes, how often does it predict yes.
-False Positive Rate: When it's actually no, how often does it predict yes.
-Specificity: When it's actually no, how often does it predict no.
-Precision: When it predicts yes, how often is it correct.
-Prevalence: How often does the yes condition actually occur in our sample.


### Coding Procedure

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Our coding procedure is outlined below:

```{r}
# Set seed for consistent results
set.seed(1)

# Importing Libraries
library(readr) # Importing and exporting csv files
library(stringi) # Fast string manipulation package (improved version of stringr)
library(quanteda) # Fast tokenizing and dfm creation (see methods)
library(dplyr) # Tidy variable creator and data cleaner
library(caret) # Predictive modeling package, used for confusionMatrix function (see methods) 
library(glmnet) # Predictive modeling package, used for cross validated logistic regression
library(doParallel) # Parallelizing model and dfm building
registerDoParallel(4) # Number of cores to be parallel over
library(tidytext) # Tidy data wrangling, used for stopwords
library(data.table) # Importing swear databases that are friendly with sentimentr
library(sentimentr) # Apply custom sentiments to words of interest

# Import dataset from Kaggle.com
train = read_csv("train.csv")

# Making the number of rows even, dropping the first observation
train = train[2:159571,]

# Splitting the dataset into a training and a testing set
sample = sample(nrow(train), nrow(train)/2)
test = train[-sample,]
train = train[sample,]
idtrain = 1:nrow(train)
idtest = (nrow(train)+1):(nrow(train) + nrow(test))
traintest = rbind(train[,1:2], test[1:2])
gc()

# Using dplyr and stringi to generate new variables related to properties of the strings
traintest = rbind(train[,1:2], test[1:2])
traintest = mutate(traintest,
       length = stri_length(comment_text),
       ncap = stri_count_charclass(comment_text, "[A-Z]"),
       nnum = stri_count_charclass(comment_text, "[0-9]"),
       ncap_len = ncap / length,
       nnum_len = nnum / length,
       nexcl = stri_count_fixed(comment_text, "!"),
       nquest = stri_count_fixed(comment_text, "?"),
       npunct = stri_count_charclass(comment_text, "[[:punct:]]"),
       npunct_len = npunct / length,
       nsent = stri_count_boundaries(comment_text, "sentence"),
       nsymb = stri_count_regex(comment_text, "&|@|#|\\$|%|\\*|\\^"),
       nsmile = stri_count_regex(comment_text, "((?::|;|=)(?:-)?(?:\\)|D|P))"),
       nwords = stri_count_words(comment_text)
       )
train = cbind(train, traintest[idtrain,3:ncol(traintest)])
test = cbind(test, traintest[idtest,3:ncol(traintest)])

###swears
swears<-fread("swears.csv", sep = ",")
hatewords<-fread("hatebase_dict1.csv",sep=",")
#swears2 <-fread("swearlistcscmu.csv",sep=",")
#swears3<-fread("googleswear.csv",sep=",")


swears<-unique( swears[ , 1:2 ] )
swears$x <- gsub(",","",swears$x)
#swears<-rbind(swears,swears2)
#swears<-rbind(swears,swears3)
swears<-unique(swears[,1:2])
mykey.swear<-update_key(swears)


hatewords<-unique(hatewords[,1:2])
hatewords$words<-gsub(",","",hatewords$words)
hatewords$words<-gsub("'"," ",hatewords$words)

hatewords$words<-tolower(hatewords$words)
hatewords
colnames(hatewords)[1] <- "x"
colnames(hatewords)[2]<-"y"
hatewords<-unique(hatewords[,1:2])
key.hatewords <- update_key(hatewords)

stopWords = c(stop_words$word, "of", "or", "on")
stopWords=stopWords[!(stopWords %in% c("new","used","small","large","thanks","greetings","works"))]

cmmnt = train$comment_text
cmmnt = unlist(cmmnt)[!(unlist(cmmnt) %in% stopWords)]
cmmnt = stri_replace_all_charclass(cmmnt, "[^[:alnum:]]", " ")
id = rep(1:nrow(train))
cmmnt_df = data.frame(id = id, comments = cmmnt)
cmmnt_df$comments = as.character(cmmnt_df$comments)


cmmnt.t = test$comment_text
cmmnt.t = unlist(cmmnt.t)[!(unlist(cmmnt.t) %in% stopWords)]
cmmnt.t = stri_replace_all_charclass(cmmnt.t, "[^[:alnum:]]", " ")
id = rep(1:nrow(test))
cmmnt_df.t = data.frame(id = id, comments = cmmnt.t)
cmmnt_df.t$comments = as.character(cmmnt_df.t$comments)


swear.sent.train<-sentiment_by(cmmnt_df$comments,list(id), polarity_dt=mykey.swear)
swear.sent.test<-sentiment_by(cmmnt_df.t$comments,list(id),polarity_dt=mykey.swear)

colnames(swear.sent.test)[4] <- "swear_sent"
colnames(swear.sent.train)[4]<-"swear_sent"


#cmmnt_df$comments<-na.omit(cmmnt_df$comments)
#cmmnt_df.t$comments<-na.omit(cmmnt_df.t$comments)

#hate.sent.train<-sentiment_by(cmmnt_df$comments,list(id), polarity_dt=key.hatewords)
#hate.sent.test<-sentiment_by(cmmnt_df.t$comments,list(id),polarity_dt=key.hatewords)

#colnames(hate.sent.test)[4] <- "hate_sent"
#colnames(hate.sent.train)[4]<-"hate_sent"

train<-cbind(train,swear.sent.train[,4])
test<-cbind(test,swear.sent.test[,4])
#train<-cbind(train,hate.sent.train[,4])
#test<-cbind(test,hate.sent.test[,4])
###positive sent- afinn
pos.sent<-sentiment_by(cmmnt_df$comments, list(id), lexicon="afinn")
pos.sent.t<-sentiment_by(cmmnt_df$comments, list(id), lexicon="afinn")

colnames(pos.sent)[4] <- "afinn"
colnames(pos.sent.t)[4]<-"afinn"

train<-cbind(train,pos.sent[,4])
test<-cbind(test,pos.sent.t[,4])


## quanteda
ttcorp = corpus(traintest$comment_text)
ttdfm = dfm(ttcorp, remove = stopwords("english"))
ttdfm = dfm_trim(ttdfm, min_docfreq = 150, max_docfreq = 100000)
ttdfm = dfm_sort(ttdfm)

# sparse matrix
sumttdfm = summary(ttdfm)
sparsettdfm = sparseMatrix(i = sumttdfm$i, j = sumttdfm$j, x = sumttdfm$x)

sparsetrain = Matrix(as.matrix(train[3:23]), sparse = TRUE)
sparsetest = Matrix(as.matrix(test[3:23]), sparse = TRUE)
train = cbind(sparsetrain, sparsettdfm[1:nrow(train),])
test = cbind(sparsetest, sparsettdfm[(nrow(train)+1):(nrow(train) + nrow(test)),])
rm(sparsetrain, sparsetest)

toxicglm = cv.glmnet(train[,8:dim(train)[2]], 
                     factor(train[,1]), 
                     alpha = 0, 
                     family = "binomial", 
                     type.measure = "auc",
                     parallel = T,
                     nfolds = 40,
                     standardize = T,
                     nlambda = 50)
toxicpreds = predict(toxicglm, newx = test[,8:dim(test)[2]], type = "response", s = "lambda.min")
toxicpreds = ifelse(toxicpreds > 0.27, 1 ,0)
toxicconf = confusionMatrix(factor(toxicpreds), factor(test[,1]))

severe_toxicglm = cv.glmnet(train[,8:dim(train)[2]], 
                     factor(train[,2]), 
                     alpha = 0, 
                     family = "binomial", 
                     type.measure = "auc",
                     parallel = T,
                     nfolds = 4,
                     standardize = T,
                     nlambda = 50)
severe_toxicpreds = predict(severe_toxicglm, newx = test[,8:dim(test)[2]], type = "response", s = "lambda.min")
severe_toxicpreds = ifelse(severe_toxicpreds > 0.02, 1 ,0)
severe_toxicconf = confusionMatrix(factor(severe_toxicpreds), factor(test[,2]))

obsceneglm = cv.glmnet(train[,8:dim(train)[2]], 
                     factor(train[,3]), 
                     alpha = 0, 
                     family = "binomial", 
                     type.measure = "auc",
                     parallel = T,
                     nfolds = 4,
                     standardize = T,
                     nlambda = 50)
obscenepreds = predict(obsceneglm, newx = test[,8:dim(test)[2]], type = "response", s = "lambda.min")
obscenepreds = ifelse(obscenepreds > 0.22, 1 ,0)
obsceneconf = confusionMatrix(factor(obscenepreds), factor(test[,3]))

threatglm = cv.glmnet(train[,8:dim(train)[2]], 
                     factor(train[,4]), 
                     alpha = 0, 
                     family = "binomial", 
                     type.measure = "auc",
                     parallel = T,
                     nfolds = 4,
                     standardize = T,
                     nlambda = 50)
threatpreds = predict(threatglm, newx = test[,8:dim(test)[2]], type = "response", s = "lambda.min")
threatpreds = ifelse(threatpreds > 1, 1 ,0)
threatconf = confusionMatrix(factor(threatpreds), factor(test[,4]))

insultglm = cv.glmnet(train[,8:dim(train)[2]], 
                     factor(train[,5]), 
                     alpha = 0, 
                     family = "binomial", 
                     type.measure = "auc",
                     parallel = T,
                     nfolds = 4,
                     standardize = T,
                     nlambda = 50)
insultpreds = predict(insultglm, newx = test[,8:dim(test)[2]], type = "response", s = "lambda.min")
insultpreds = ifelse(insultpreds > 0.21, 1 ,0)
insultconf = confusionMatrix(factor(insultpreds), factor(test[,5]))

identity_hateglm = cv.glmnet(train[,8:dim(train)[2]], 
                     factor(train[,6]), 
                     alpha = 0, 
                     family = "binomial", 
                     type.measure = "auc",
                     parallel = T,
                     nfolds = 4,
                     standardize = T,
                     nlambda = 50)
identity_hatepreds = predict(identity_hateglm, newx = test[,8:dim(test)[2]], type = "response", s = "lambda.min")
identity_hatepreds = ifelse(identity_hatepreds > 1, 1 ,0)
identity_hateconf = confusionMatrix(factor(identity_hatepreds), factor(test[,6]))
```

### Results

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The study created 23 predictive variables: Length ( number of characters), ncap(number of capital letters), nnum(number of numbers), ncap_len(ratio of ncap to Length),nnum_len(ratio of nnum to length), nexcl(number of exclamation marks), nquest( number of question marks), npunct( number of punctuation marks defined by [[:punct]] lexicon), npunct_len( punctuation to length ratio), nsent (number of sentences), nsymb ( number of symbols “ (&|@|#|\\$|%|\\*|\\^ )”), nsmile (number of smiley faces “(?::|;|=)(?:-)?(?:\\)|D|P)”), nwords ( number of words), hate ( number of occurrences of “ni****” and “fa*”), nhate ( number of occurrences of hate words defined by custom lexicon),  AFINN sentiment value, Custom swear sentiment value, hate words sentiment value.  Additionally a document feature matrix was added to predictive variables which consisted of 4000 features that were counts of different single element components of a text.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; These predictors were used with a cross validated logistic regression to classify comments as Toxic, Severe_Toxic, Obscene, Threat, Insult, and Identity Hate.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The results are evaluated as percent of accurate classifications made and p-values are given as an improvement from the no-information rate.

> Toxic: `r unname((toxicconf$overall[1]-toxicconf$overall[5])/(1-toxicconf$overall[5]))*100`, 

> Severe_Toxic: `r unname((severe_toxicconf$overall[1]-severe_toxicconf$overall[5])/(1-severe_toxicconf$overall[5]))*100`, 

> Obscene: `r unname((obsceneconf$overall[1]-obsceneconf$overall[5])/(1-obsceneconf$overall[5]))*100`, 

> Threat: `r unname((threatconf$overall[1]-threatconf$overall[5])/(1-threatconf$overall[5]))*100`, 

> Insult: `r unname((insultconf$overall[1]-insultconf$overall[5])/(1-insultconf$overall[5]))*100`, 

> Hate: `r unname((identity_hateconf$overall[1]-identity_hateconf$overall[5])/(1-identity_hateconf$overall[5]))*100`.

Toxic = `r unname((toxicconf$overall[1]-toxicconf$overall[5])/(1-toxicconf$overall[5]))*100`
Severe_Toxic = `r unname((severe_toxicconf$overall[1]-severe_toxicconf$overall[5])/(1-severe_toxicconf$overall[5]))*100`
Obscene = `r unname((obsceneconf$overall[1]-obsceneconf$overall[5])/(1-obsceneconf$overall[5]))*100`
Threat = `r unname((threatconf$overall[1]-threatconf$overall[5])/(1-threatconf$overall[5]))*100`
Insult = `r unname((insultconf$overall[1]-insultconf$overall[5])/(1-insultconf$overall[5]))*100`
Identity_Hate = `r unname((identity_hateconf$overall[1]-identity_hateconf$overall[5])/(1-identity_hateconf$overall[5]))*100`

### Discussion

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The results indicate that statistically significant classifications can be made by analysis of comment structure and sentiment analysis.  This combination was found to be an improvement over each method independently  method and substantiates intuition that Natural Language Processing is multi-facetted.  The study recognizes that further relationships between structure, sentiment, and other factors could play an important role in accurate comment classifications. 

### Conclusion

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This study combined techniques commonly used for text inference in order to make predict classifications.  The ability to draw meaningful inference from vasts amount of text data is inextricably connected to being able to make predictions and classify text.  This study explored a method of examining text structure and word sentiment to classify comments and produce insight from large amounts of text data.

### Acknowledgements

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This study was facilitated by our faculty advisor Sean Raleigh and was supported by Westminster College. We’d also like to thank Kaggle for hosting this data and creating the competition that this data was drawn from. Additionally, we’d be remiss to not acknowledge Wikipedia’s role in providing the data and the incentives on Kaggle.

### Works Cited

Chowdhury, Gobinda G. “Natural Language Processing.” Annual Review of Information 
Science and Technology, Wiley-Blackwell, 31 Jan. 2005, 
onlinelibrary.wiley.com/doi/full/10.1002/aris.1440370103.

Silge, Julia & Robinson, David. “Tidy Mining with R, A Tidy Approach”.

https://science.howstuffworks.com/life/evolution/language-evolve.htm

https://insidebigdata.com/2017/02/16/the-exponential-growth-of-data/

