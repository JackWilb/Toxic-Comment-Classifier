---
title: "UGRF"
output: pdf_document
---
### Libraries


```{r, message=FALSE, warning=FALSE}
library(knitr)
library(stringi)
library(stringr)
library(readr)
library(quanteda)
library(dplyr)
library(glmnet)
library(caret)
#library(doParallel)
library(mosaic)
library(broom)
library(cleanNLP)
library(tm)
library(tidytext)
library(sentimentr)
library(data.table)
library(magrittr)
library(randomForest)
library(class)
library(e1071)
library(nnet)
library(neuralnet)
library(randomForest)
library(ranger)
library(ISLR)
library(boot)
library(syuzhet)
library(RTextTools)
library(ISLR)
library(tidyverse)
library(caret)
library(rpart)
library(rpart.plot)
library(MASS)
library(gbm)
library(tree)
library(class)
```

##Import data.

```{r} 
toxic.train = fread("~/Documents/Rstudio/Toxic-Comment-Classifier/train.csv", sep=',')
toxic.train<-toxic.train[1:20000,]
toxic<-createDataPartition(toxic.train$toxic,p=.75,list=FALSE)
train<-toxic.train[toxic,]
test<-toxic.train[-toxic,]
```
##Exploratory
Now that we have some additional variables lets explore the data a bit.  
First lets count what we have.
```{r}
tally(train$toxic)
tally(train$severe_toxic)
tally(train$obscene)
tally(train$threat)
tally(train$insult)
tally(train$identity_hate)
```
The tally command tells us exactly how many of each category there are so we know what we are looking at.  Initial observations show that some categories like "threat" are quite small which may make classifying "threat" comments difficult. Let's take a look at the most common words from each category using tidy text.


##Tidy Text Word Analysis
```{r, warning=FALSE}
traincorpus = corpus(train$comment_text)
docvars(traincorpus, "id") = 1:nrow(train)
docvars(traincorpus, "toxic") = train$toxic
docvars(traincorpus, "severe_toxic") = train$severe_toxic
docvars(traincorpus, "obscene") = train$obscene
docvars(traincorpus, "threat") = train$threat
docvars(traincorpus, "insult") = train$insult
docvars(traincorpus, "identity_hate") = train$identity_hate
docvars(traincorpus, "ntokens") = ntoken(traincorpus, remove_punct = TRUE)





traindfm2 = dfm(traincorpus, remove = stopwords("english"), stem = TRUE, remove_punct = TRUE)


toxicdfm2 = dfm_subset(traindfm2, toxic == 1, remove = stopwords("english"))
toxicfeatures2 = topfeatures(toxicdfm2, 25)
toxicwords2 = names(toxicfeatures2)

severe_toxicdfm2 = dfm_subset(traindfm2, severe_toxic == 1,remove = stopwords("english"))
severe_toxicfeatures2 = topfeatures(severe_toxicdfm2, 10)
severe_toxicwords2 = names(severe_toxicfeatures2)

obscenedfm2 = dfm_subset(traindfm2, obscene == 1,remove = stopwords("english"))
obscenefeatures2 = topfeatures(obscenedfm2, 10)
obscenewords2 = names(obscenefeatures2)

threatdfm2 = dfm_subset(traindfm2, threat == 1,remove = stopwords("english"))
threatfeatures2 = topfeatures(threatdfm2, 10)
threatwords2 = names(threatfeatures2)

insultdfm2 = dfm_subset(traindfm2, insult == 1,remove = stopwords("english"))
insultfeatures2 = topfeatures(insultdfm2, 10)
insultwords2 = names(insultfeatures2)

identity_hatedfm2 = dfm_subset(traindfm2, identity_hate == 1,remove = stopwords("english"))
identity_hatefeatures2 = topfeatures(identity_hatedfm2, 10)
identity_hatewords2 = names(identity_hatefeatures2)



data.frame(term = names(toxicfeatures2), freq = unname(toxicfeatures2)) %>%
    ggplot(aes(x = reorder(term, freq), y = freq)) + 
    geom_bar(stat = 'identity', fill = 'orangered2') + 
    labs(x = '', y = 'Frequency', title = 'Most common Toxic Words') + 
    coord_flip() 

data.frame(term = names(severe_toxicfeatures2), freq = unname(severe_toxicfeatures2)) %>%
    ggplot(aes(x = reorder(term, freq), y = freq)) + 
    geom_bar(stat = 'identity', fill = 'orangered2') + 
    labs(x = '', y = 'Frequency', title = 'Most common Severe_toxic Words ') + 
    coord_flip() 

data.frame(term = names(obscenefeatures2), freq = unname(obscenefeatures2)) %>%
    ggplot(aes(x = reorder(term, freq), y = freq)) + 
    geom_bar(stat = 'identity', fill = 'orangered2') + 
    labs(x = '', y = 'Frequency', title = 'Most common Obscene Words') + 
    coord_flip() 

data.frame(term = names(threatfeatures2), freq = unname(threatfeatures2)) %>%
    ggplot(aes(x = reorder(term, freq), y = freq)) + 
    geom_bar(stat = 'identity', fill = 'orangered2') + 
    labs(x = '', y = 'Frequency', title = 'Most common Threat Word') + 
    coord_flip() 

data.frame(term = names(insultfeatures2), freq = unname(insultfeatures2)) %>%
    ggplot(aes(x = reorder(term, freq), y = freq)) + 
    geom_bar(stat = 'identity', fill = 'orangered2') + 
    labs(x = '', y = 'Frequency', title = 'Most common Insult Word') + 
    coord_flip() 

data.frame(term = names(identity_hatefeatures2), freq = unname(identity_hatefeatures2)) %>%
    ggplot(aes(x = reorder(term, freq), y = freq)) + 
    geom_bar(stat = 'identity', fill = 'orangered2') + 
    labs(x = '', y = 'Frequency', title = 'Most common Identity_hate Word') + 
    coord_flip() 
```

##PCA
```{r}
train_num<-select_if(train, is.numeric)
str(train_num)


row.names(train_num)<-paste(train$toxic,train)
head(train_num)
train_num<-na.omit(train_num)


train_pca<- prcomp(train_num, scale=TRUE)
##Scale = true standardizes the data, mean centers an sd=1
summary(train_pca)
##cumulative proportion is how much of the data is explained.

train_pca_tidy<- tidy(train_pca, matrix = "variables")
head(train_pca_tidy, n=20)
#gives us data in long format= a single colum with all possible values
#try wide format
#opposite of spread is gather.  wide to long. 
train_pca_tidy_wide<- spread(train_pca_tidy, key=PC, value=value)

head(cars_pca_tidy_wide)
##numbers are loadings


#how to get to each point in space
train_pca_aug<-augment(train_pca)
head(train_pca_aug)
```
```{r fig.width=14, fig.height=14}
plot(train_pca)
biplot(train_pca)

#var.axes = TRUE,
       xlabs = FALSE, ylabs = FALSE, expand = 1,
       xlim  = FALSE, ylim  = FALSE, arrow.len = 0.1,
       main = FALSE, sub = FALSE, xlab = FALSE, ylab = FALSE)
autoplot(train_pca,data=train_num, loadings = TRUE, label=TRUE, loadings.label=TRUE)

##PC1 says it explains 63% of variation  pc2 13.  

##better model - 

```
